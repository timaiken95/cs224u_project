{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_json('train.json')\n",
    "data_val = pd.read_json('val.json')\n",
    "MAX_SEQUENCE_LENGTH = data_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.load(\"GloVe_codeswitch_5k.npy\")\n",
    "words = np.load('5k_vocab_dict.npy').item()\n",
    "EMBEDDING_DIM = len(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = \"eng\"\n",
    "spanish = \"span\"\n",
    "other = \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createExamplesLabels(data):\n",
    "    examples = []\n",
    "    labels = []\n",
    "    num_reviews, review_length = data.shape\n",
    "\n",
    "    for r in range(num_reviews):\n",
    "        review_string = \"\"\n",
    "        label_vec = []\n",
    "\n",
    "        for w in range(review_length):\n",
    "\n",
    "            currWordStruct = data[w][r]\n",
    "\n",
    "            if currWordStruct == None:\n",
    "                break\n",
    "                \n",
    "            currWord = currWordStruct[0]\n",
    "\n",
    "            if currWord in words:\n",
    "                review_string += (\" \" + currWord)\n",
    "            else:\n",
    "                review_string += (\" <UNK>\")\n",
    "\n",
    "            if w < (review_length - 1):\n",
    "                nextWordStruct = data[w + 1][r]\n",
    "                if nextWordStruct:\n",
    "\n",
    "                    nextWord = nextWordStruct[0]\n",
    "                    nextLang = nextWordStruct[1]\n",
    "\n",
    "                    if nextLang == 'eng':\n",
    "                        label_vec.append(english)\n",
    "\n",
    "                    elif nextLang == 'spa':\n",
    "                        label_vec.append(spanish)\n",
    "\n",
    "                    elif nextLang == 'eng&spa' or 'eng+spa' or 'spa+eng':\n",
    "                        label_vec.append(other)\n",
    "\n",
    "                else:\n",
    "                    label_vec.append(other)\n",
    "\n",
    "        labels.append(label_vec)\n",
    "        examples.append(review_string)\n",
    "    \n",
    "    return examples, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_train, labels_train = createExamplesLabels(data_train)\n",
    "examples_val, labels_val = createExamplesLabels(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=len(vectors), filters=\"\", lower=False)\n",
    "tokenizer.fit_on_texts(examples_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(examples_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(examples_val)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train_data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val_data = pad_sequences(sequences_val, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "for k,v in words.items():\n",
    "    embedding_dict[k] = vectors[v]\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_dict.get(word)\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([other, english, spanish])\n",
    "label_transform_train = np.zeros((len(labels_train), MAX_SEQUENCE_LENGTH, 3))\n",
    "for i, vec in enumerate(labels_train):\n",
    "\n",
    "    curr = to_categorical(pad_sequences([le.transform(vec)], maxlen=MAX_SEQUENCE_LENGTH), num_classes = 3)[0]\n",
    "    label_transform_train[i,:,:] = curr\n",
    "\n",
    "label_transform_val = np.zeros((len(labels_val), MAX_SEQUENCE_LENGTH, 3))\n",
    "for i, vec in enumerate(labels_val):\n",
    "\n",
    "    curr = to_categorical(pad_sequences([le.transform(vec)], maxlen=MAX_SEQUENCE_LENGTH), num_classes = 3)[0]\n",
    "    label_transform_val[i,:,:] = curr\n",
    "    \n",
    "#keys = list(le.classes_)\n",
    "#vals = le.transform(keys)\n",
    "#labels_index = dict(zip(keys,vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 61, 300)           1500300   \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 61, 200)           400800    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 61, 3)             603       \n",
      "=================================================================\n",
      "Total params: 1,901,703\n",
      "Trainable params: 401,403\n",
      "Non-trainable params: 1,500,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 31967 samples, validate on 1000 samples\n",
      "Epoch 1/6\n",
      "31967/31967 [==============================] - 89s 3ms/step - loss: 0.1275 - acc: 0.9720 - val_loss: 0.1270 - val_acc: 0.9492\n",
      "Epoch 2/6\n",
      "31967/31967 [==============================] - 88s 3ms/step - loss: 0.0683 - acc: 0.9740 - val_loss: 0.1164 - val_acc: 0.9508\n",
      "Epoch 3/6\n",
      "31967/31967 [==============================] - 84s 3ms/step - loss: 0.0565 - acc: 0.9770 - val_loss: 0.1007 - val_acc: 0.9582\n",
      "Epoch 4/6\n",
      "31967/31967 [==============================] - 87s 3ms/step - loss: 0.0454 - acc: 0.9816 - val_loss: 0.1045 - val_acc: 0.9544\n",
      "Epoch 5/6\n",
      "31967/31967 [==============================] - 105s 3ms/step - loss: 0.0400 - acc: 0.9839 - val_loss: 0.1001 - val_acc: 0.9623\n",
      "Epoch 6/6\n",
      "31967/31967 [==============================] - 86s 3ms/step - loss: 0.0358 - acc: 0.9856 - val_loss: 0.0983 - val_acc: 0.9630\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(200, return_sequences=True, name=\"LSTM\"))\n",
    "model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "#model.train_on_batch(data[4:8,:], label_transform[4:8,:,:])\n",
    "#model.train_on_batch(data[8:12,:], label_transform[8:12,:,:])\n",
    "#model.predict_on_batch(data[10:20,:])\n",
    "results = model.fit(train_data, label_transform_train, epochs=6, validation_data = (val_data, label_transform_val), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.12704526036977767, 0.11638797223567962, 0.10066358149051666, 0.1044807754456997, 0.10011781677603722, 0.09831734970211983], 'val_acc': [0.9491803169250488, 0.950803279876709, 0.9582131266593933, 0.9543934464454651, 0.9622622966766358, 0.9629508256912231], 'loss': [0.12747760998388658, 0.0682584139622943, 0.05653371496036498, 0.04541250440091707, 0.03997279140552525, 0.03578736492392362], 'acc': [0.9720187910311581, 0.9740167519624618, 0.9770362580386013, 0.9816419272653699, 0.9838609187973748, 0.9855701578742261]}\n"
     ]
    }
   ],
   "source": [
    "print(results.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
