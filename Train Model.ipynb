{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_json('train.json')\n",
    "data_val = pd.read_json('val.json')\n",
    "MAX_SEQUENCE_LENGTH = data_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.load(\"GloVe_codeswitch_5k.npy\")\n",
    "words = np.load('5k_vocab_dict.npy').item()\n",
    "EMBEDDING_DIM = len(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = \"eng\"\n",
    "spanish = \"span\"\n",
    "other = \"other\"\n",
    "\n",
    "switch = \"switch\"\n",
    "noswitch = \"noswitch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createExamplesLabels(data):\n",
    "    examples = []\n",
    "    labels = []\n",
    "    num_reviews, review_length = data.shape\n",
    "\n",
    "    for r in range(num_reviews):\n",
    "        review_string = \"\"\n",
    "        label_vec = []\n",
    "\n",
    "        for w in range(review_length):\n",
    "\n",
    "            currWordStruct = data[w][r]\n",
    "\n",
    "            if currWordStruct == None:\n",
    "                break\n",
    "                \n",
    "            currWord = currWordStruct[0]\n",
    "\n",
    "            if currWord in words:\n",
    "                review_string += (\" \" + currWord)\n",
    "            else:\n",
    "                review_string += (\" <UNK>\")\n",
    "\n",
    "            if w < (review_length - 1):\n",
    "                nextWordStruct = data[w + 1][r]\n",
    "                if nextWordStruct:\n",
    "\n",
    "                    nextWord = nextWordStruct[0]\n",
    "                    nextLang = nextWordStruct[1]\n",
    "\n",
    "                    if nextLang == 'eng':\n",
    "                        label_vec.append(english)\n",
    "\n",
    "                    elif nextLang == 'spa':\n",
    "                        label_vec.append(spanish)\n",
    "\n",
    "                    elif nextLang == 'eng&spa' or 'eng+spa' or 'spa+eng':\n",
    "                        label_vec.append(other)\n",
    "\n",
    "                else:\n",
    "                    label_vec.append(other)\n",
    "\n",
    "        labels.append(label_vec)\n",
    "        examples.append(review_string)\n",
    "    \n",
    "    return examples, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createExamplesBinary(data):\n",
    "    examples = []\n",
    "    labels = []\n",
    "    num_reviews, review_length = data.shape\n",
    "\n",
    "    for r in range(num_reviews):\n",
    "        review_string = \"\"\n",
    "        label_vec = []\n",
    "        flag = -1\n",
    "\n",
    "        for w in range(review_length):\n",
    "\n",
    "            currWordStruct = data[w][r]\n",
    "\n",
    "            if currWordStruct == None:\n",
    "                break\n",
    "                \n",
    "            currWord = currWordStruct[0]\n",
    "            currLang = currWordStruct[1]\n",
    "\n",
    "            if currWord in words:\n",
    "                review_string += (\" \" + currWord)\n",
    "            else:\n",
    "                review_string += (\" <UNK>\")\n",
    "\n",
    "            if w < (review_length - 1):\n",
    "                nextWordStruct = data[w + 1][r]\n",
    "                if nextWordStruct:\n",
    "\n",
    "                    nextWord = nextWordStruct[0]\n",
    "                    nextLang = nextWordStruct[1]\n",
    "\n",
    "                    if currLang != nextLang:\n",
    "                        label_vec.append(switch)\n",
    "                        flag = 0\n",
    "                        \n",
    "                    elif flag >= 0:\n",
    "                        \n",
    "                        if flag == 4:\n",
    "                            flag = -1\n",
    "                        else:\n",
    "                            flag += 1\n",
    "                            \n",
    "                        label_vec.append(switch)\n",
    "\n",
    "                    else:\n",
    "                        label_vec.append(noswitch)\n",
    "\n",
    "                else:\n",
    "                    label_vec.append(noswitch)\n",
    "\n",
    "        labels.append(label_vec)\n",
    "        examples.append(review_string)\n",
    "    \n",
    "    return examples, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_train, labels_train = createExamplesBinary(data_train)\n",
    "examples_val, labels_val = createExamplesBinary(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=len(vectors), filters=\"\", lower=False)\n",
    "tokenizer.fit_on_texts(examples_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(examples_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(examples_val)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "train_data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "val_data = pad_sequences(sequences_val, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "for k,v in words.items():\n",
    "    embedding_dict[k] = vectors[v]\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_dict.get(word)\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "#le.fit([other, english, spanish])\n",
    "le.fit([switch, noswitch])\n",
    "label_transform_train = np.zeros((len(labels_train), MAX_SEQUENCE_LENGTH, 2))\n",
    "for i, vec in enumerate(labels_train):\n",
    "\n",
    "    curr = to_categorical(pad_sequences([le.transform(vec)], maxlen=MAX_SEQUENCE_LENGTH), num_classes = 2)[0]\n",
    "    label_transform_train[i,:,:] = curr\n",
    "\n",
    "label_transform_val = np.zeros((len(labels_val), MAX_SEQUENCE_LENGTH, 2))\n",
    "for i, vec in enumerate(labels_val):\n",
    "\n",
    "    curr = to_categorical(pad_sequences([le.transform(vec)], maxlen=MAX_SEQUENCE_LENGTH), num_classes = 2)[0]\n",
    "    label_transform_val[i,:,:] = curr\n",
    "    \n",
    "#keys = list(le.classes_)\n",
    "#vals = le.transform(keys)\n",
    "#labels_index = dict(zip(keys,vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3653\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.argmax(label_transform_val.reshape(-1, 2), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = np.argmax((np.asarray(self.model.predict(val_data))).reshape(-1, 2), axis=1)\n",
    "        val_targ = np.argmax(label_transform_val.reshape(-1, 2), axis=1)\n",
    "        print(np.sum(val_predict))\n",
    "        _val_f1 = f1_score(val_targ, val_predict, average='binary')\n",
    "        _val_recall = recall_score(val_targ, val_predict, average='binary')\n",
    "        _val_precision = precision_score(val_targ, val_predict, average='binary')\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\"— val_f1: %f — val_precision: %f — val_recall %f\" % (_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "    \n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 52, 300)           1326300   \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 52, 1000)          5204000   \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 52, 2)             2002      \n",
      "=================================================================\n",
      "Total params: 6,532,302\n",
      "Trainable params: 5,206,002\n",
      "Non-trainable params: 1,326,300\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3544 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "3544/3544 [==============================] - 127s 36ms/step - loss: 0.3487 - acc: 0.9315 - val_loss: 0.1790 - val_acc: 0.9297\n",
      "2822\n",
      "— val_f1: 0.435521 — val_precision: 0.499646 — val_recall 0.385984\n",
      "Epoch 2/20\n",
      "3544/3544 [==============================] - 114s 32ms/step - loss: 0.1428 - acc: 0.9343 - val_loss: 0.1605 - val_acc: 0.9302\n",
      "169\n",
      "— val_f1: 0.049712 — val_precision: 0.562130 — val_recall 0.026006\n",
      "Epoch 3/20\n",
      "3544/3544 [==============================] - 110s 31ms/step - loss: 0.1287 - acc: 0.9368 - val_loss: 0.1447 - val_acc: 0.9342\n",
      "3193\n",
      "— val_f1: 0.500146 — val_precision: 0.536173 — val_recall 0.468656\n",
      "Epoch 4/20\n",
      "3544/3544 [==============================] - 104s 29ms/step - loss: 0.1178 - acc: 0.9412 - val_loss: 0.1425 - val_acc: 0.9356\n",
      "3561\n",
      "— val_f1: 0.535902 — val_precision: 0.542825 — val_recall 0.529154\n",
      "Epoch 5/20\n",
      "3544/3544 [==============================] - 106s 30ms/step - loss: 0.1154 - acc: 0.9414 - val_loss: 0.1397 - val_acc: 0.9334\n",
      "4922\n",
      "— val_f1: 0.596385 — val_precision: 0.519504 — val_recall 0.699973\n",
      "Epoch 6/20\n",
      "1600/3544 [============>.................] - ETA: 57s - loss: 0.1086 - acc: 0.9441"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(1000, return_sequences=True, name=\"LSTM\"))\n",
    "model.add(TimeDistributed(Dense(2, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "#model.train_on_batch(data[4:8,:], label_transform[4:8,:,:])\n",
    "#model.train_on_batch(data[8:12,:], label_transform[8:12,:,:])\n",
    "#model.predict_on_batch(data[10:20,:])\n",
    "results = model.fit(train_data, label_transform_train, epochs=20, validation_data = (val_data, label_transform_val), batch_size=100, callbacks=[metrics])\n",
    "#results = model.fit(val_data, label_transform_val, epochs=6, validation_data = (val_data, label_transform_val), batch_size=100, callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.12704526036977767, 0.11638797223567962, 0.10066358149051666, 0.1044807754456997, 0.10011781677603722, 0.09831734970211983], 'val_acc': [0.9491803169250488, 0.950803279876709, 0.9582131266593933, 0.9543934464454651, 0.9622622966766358, 0.9629508256912231], 'loss': [0.12747760998388658, 0.0682584139622943, 0.05653371496036498, 0.04541250440091707, 0.03997279140552525, 0.03578736492392362], 'acc': [0.9720187910311581, 0.9740167519624618, 0.9770362580386013, 0.9816419272653699, 0.9838609187973748, 0.9855701578742261]}\n"
     ]
    }
   ],
   "source": [
    "print(results.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
